{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introducción a Análisis de Datos, Machine Learning, Inteligencia Artificial y Redes Neuronales\n",
    "\n",
    "# Álgebra lineal\n",
    "## 1. Vectores\n",
    "\"Un vector es un objeto matemático que tiene magnitud y dirección, representado como una lista de números que indican su posición en el espacio\" (Strang, Gilbert. 2016. \"Introduction to Linear Algebra\". Wellesley-Cambridge Press).\n",
    "\n",
    "## 2. Matrices\n",
    "\"Una matriz es una disposición rectangular de números en filas y columnas, utilizada para representar transformaciones lineales y sistemas de ecuaciones lineales\" (Lay, David C., Steven R. Lay, and Judi J. McDonald. 2016. \"Linear Algebra and Its Applications\". Pearson).\n",
    "\n",
    "## 3. Multiplicación de Matrices\n",
    "\"La multiplicación de matrices es una operación que toma un par de matrices y produce otra matriz, representando la composición de dos transformaciones lineales\" (Horn, Roger A., and Charles R. Johnson. 2012. \"Matrix Analysis\". Cambridge University Press).\n",
    "\n",
    "# Cálculo\n",
    "## 1. Derivadas\n",
    "\"La derivada de una función mide la tasa de cambio de la función con respecto a una variable, representando la pendiente de la tangente a la curva de la función en un punto dado\" (Stewart, James. 2015. \"Calculus: Early Transcendentals\". Cengage Learning).\n",
    "\n",
    "## 2. Gradiente\n",
    "\"El gradiente es un vector que contiene las derivadas parciales de una función con respecto a cada una de sus variables, indicando la dirección de mayor aumento de la función\" (Thomas, George B., and Ross L. Finney. 2002. \"Calculus and Analytic Geometry\". Addison-Wesley).\n",
    "\n",
    "## 3. Regla de la Cadena\n",
    "\"La regla de la cadena es una fórmula para calcular la derivada de la composición de dos o más funciones, crucial para la retropropagación en redes neuronales\" (Anton, Howard, Irl Bivens, and Stephen Davis. 2012. \"Calculus: Early Transcendentals\". Wiley).\n",
    "\n",
    "# Probabilidad y Estadística\n",
    "## 1. Distribución de Probabilidad\n",
    "\"Una distribución de probabilidad describe cómo se distribuyen los valores de una variable aleatoria, especificando las probabilidades de los diferentes resultados\" (DeGroot, Morris H., and Mark J. Schervish. 2012. \"Probability and Statistics\". Pearson).\n",
    "\n",
    "## 2. Esperanza Matemática (Media Esperada)\n",
    "\"La esperanza matemática es el valor promedio ponderado de una variable aleatoria, calculado como la suma de todos los posibles valores multiplicados por sus probabilidades\" (Ross, Sheldon M. 2014. \"Introduction to Probability Models\". Academic Press).\n",
    "\n",
    "## 3. Varianza\n",
    "\"La varianza es una medida de la dispersión de una variable aleatoria, definida como la expectativa del cuadrado de la desviación de la variable con respecto a su media\" (Rice, John A. 2007. \"Mathematical Statistics and Data Analysis\". Cengage Learning).\n",
    "\n",
    "# Optimización\n",
    "## 1. Función de Costo (Loss Function)\n",
    "\"Una función de costo es una función que mide el error o la discrepancia entre las predicciones del modelo y los valores reales, utilizada para guiar el proceso de aprendizaje del modelo\" (Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. \"Deep Learning\". MIT Press).\n",
    "\n",
    "## 2. Gradiente Descendente (Gradient Descent)\n",
    "\"El gradiente descendente es un algoritmo de optimización que ajusta iterativamente los parámetros de un modelo en la dirección opuesta al gradiente de la función de costo para minimizarla\" (Bishop, Christopher M. 2006. \"Pattern Recognition and Machine Learning\". Springer).\n",
    "\n",
    "## 3. Regularización\n",
    "\"La regularización es una técnica utilizada para prevenir el sobreajuste en modelos de aprendizaje automático añadiendo una penalización a la función de costo basada en la magnitud de los parámetros del modelo\" (Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. \"The Elements of Statistical Learning: Data Mining, Inference, and Prediction\". Springer).\n",
    "\n",
    "\n",
    "# Teoría de la Información\n",
    "## 1. Entropía\n",
    "\"La entropía es una medida de la incertidumbre en una variable aleatoria, definida como la expectativa del logaritmo negativo de la probabilidad de los eventos\" (Cover, Thomas M., and Joy A. Thomas. 2006. \"Elements of Information Theory\". Wiley-Interscience).\n",
    "\n",
    "## 2. Divergencia de Kullback-Leibler (KL Divergence)\n",
    "\"La divergencia de Kullback-Leibler es una medida de la diferencia entre dos distribuciones de probabilidad, cuantificando cuánto se desvía una distribución de otra\" (Kullback, Solomon, and Richard A. Leibler. 1951. \"On Information and Sufficiency\". Annals of Mathematical Statistics).\n",
    "\n",
    "\n",
    "# Análisis de Datos\n",
    "\n",
    "## 1. Análisis Exploratorio de Datos (EDA)\n",
    "El Análisis Exploratorio de Datos (EDA) es \"un enfoque de análisis de datos que emplea principalmente gráficos y técnicas visuales para descubrir patrones, detectar anomalías, probar una hipótesis y verificar suposiciones con la ayuda de estadísticas resumidas y representaciones gráficas\" (Yau, Nathan. 2013. \"Data Points: Visualization That Means Something\". Wiley).\n",
    "\n",
    "## 2. Preprocesamiento de Datos\n",
    "El preprocesamiento de datos es \"la etapa en la que se preparan y transforman los datos brutos en un formato adecuado para el análisis y modelado, incluyendo la limpieza de datos, la transformación de datos y la normalización de datos\" (Han, Jiawei, Micheline Kamber, and Jian Pei. 2011. \"Data Mining: Concepts and Techniques\". Elsevier).\n",
    "\n",
    "## 3. Visualización de Datos\n",
    "La visualización de datos es \"la representación gráfica de datos o información, diseñada para permitir a los usuarios entender los datos de forma clara y efectiva mediante gráficos estadísticos, gráficos de información y otras herramientas de visualización\" (Few, Stephen. 2009. \"Now You See It: Simple Visualization Techniques for Quantitative Analysis\". Analytics Press).\n",
    "\n",
    "\n",
    "\n",
    "# Machine Learning\n",
    "\n",
    "## 1. Aprendizaje Supervisado\n",
    "El aprendizaje supervisado es \"un tipo de aprendizaje automático en el que el modelo es entrenado con datos de entrada y salida etiquetados, permitiendo que el modelo aprenda a mapear las entradas a las salidas correctas\" (Bishop, Christopher M. 2006. \"Pattern Recognition and Machine Learning\". Springer).\n",
    "\n",
    "## 2. Aprendizaje No Supervisado\n",
    "El aprendizaje no supervisado es \"un tipo de aprendizaje automático en el que el modelo es entrenado con datos que no tienen etiquetas, buscando patrones y estructuras inherentes en los datos\" (Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. \"The Elements of Statistical Learning: Data Mining, Inference, and Prediction\". Springer).\n",
    "\n",
    "## 3. Aprendizaje por Refuerzo\n",
    "El aprendizaje por refuerzo es \"un tipo de aprendizaje automático en el que un agente aprende a tomar decisiones mediante la realización de acciones y la observación de los resultados de estas acciones en el entorno, recibiendo recompensas o castigos\" (Sutton, Richard S., and Andrew G. Barto. 2018. \"Reinforcement Learning: An Introduction\". MIT Press).\n",
    "\n",
    "\n",
    "# Inteligencia Artificial\n",
    "\n",
    "## 1. Inteligencia Artificial (IA)\n",
    "La inteligencia artificial es \"la capacidad de una máquina para imitar la inteligencia humana, realizando tareas como el razonamiento, el aprendizaje, la planificación y la comprensión del lenguaje natural\" (Russell, Stuart J., and Peter Norvig. 2020. \"Artificial Intelligence: A Modern Approach\". Pearson).\n",
    "\n",
    "## 2. Algoritmos Genéticos\n",
    "Los algoritmos genéticos son \"algoritmos de búsqueda y optimización inspirados en el proceso de selección natural, que utilizan operadores como mutación, cruce y selección para encontrar soluciones óptimas\" (Goldberg, David E. 1989. \"Genetic Algorithms in Search, Optimization, and Machine Learning\". Addison-Wesley).\n",
    "\n",
    "## 3. Procesamiento de Lenguaje Natural (NLP)\n",
    "El procesamiento de lenguaje natural es \"un campo de la inteligencia artificial que se ocupa de la interacción entre computadoras y lenguajes humanos, en particular, cómo programar computadoras para procesar y analizar grandes cantidades de datos de lenguaje natural\" (Jurafsky, Daniel, and James H. Martin. 2019. \"Speech and Language Processing\". Pearson).\n",
    "\n",
    "\n",
    "\n",
    "# Redes Neuronales\n",
    "\n",
    "## 1. Neurona Artificial\n",
    "Una neurona artificial es \"una unidad de procesamiento en una red neuronal que recibe uno o más inputs, aplica una función de activación y produce un output. Es una aproximación matemática del comportamiento de una neurona biológica\" (McCulloch, Warren S., and Walter Pitts. 1943. \"A Logical Calculus of Ideas Immanent in Nervous Activity\". The Bulletin of Mathematical Biophysics).\n",
    "\n",
    "## 2. Red Neuronal Artificial (ANN)\n",
    "Una red neuronal artificial es \"un sistema de computación inspirado en la estructura y funciones del cerebro humano, compuesto por unidades llamadas neuronas artificiales, que están organizadas en capas y conectadas entre sí\" (Haykin, Simon. 2009. \"Neural Networks and Learning Machines\". Pearson).\n",
    "\n",
    "## 3. Función de Activación\n",
    "La función de activación es \"una función matemática que se aplica a la salida de una neurona para introducir no linealidad en el modelo, permitiendo a la red neuronal aprender relaciones complejas en los datos\" (Nielsen, Michael A. 2015. \"Neural Networks and Deep Learning\". Determination Press).\n",
    "\n",
    "## 4. Propagación Hacia Atrás (Backpropagation)\n",
    "La propagación hacia atrás es \"un algoritmo de optimización utilizado en redes neuronales artificiales para minimizar el error de predicción ajustando los pesos de las conexiones neuronales mediante el cálculo del gradiente del error\" (Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. \"Learning Representations by Back-Propagating Errors\". Nature).\n",
    "\n",
    "## 5. Sobreajuste (Overfitting)\n",
    "El sobreajuste es \"un problema que ocurre cuando un modelo de aprendizaje automático se ajusta demasiado a los datos de entrenamiento, capturando el ruido y las fluctuaciones aleatorias, y como resultado, tiene un rendimiento deficiente en datos no vistos\" (Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. \"Deep Learning\". MIT Press).\n",
    "\n",
    "# Funciones de Activación\n",
    "## 1. Función Sigmoide\n",
    "\"La función sigmoide es una función de activación que toma cualquier valor real y lo mapea en un valor entre 0 y 1, definida como \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)\" (Nielsen, Michael A. 2015. \"Neural Networks and Deep Learning\". Determination Press).\n",
    "\n",
    "## 2. Función ReLU (Rectified Linear Unit)\n",
    "\"La función ReLU es una función de activación que devuelve 0 si la entrada es negativa y el valor de la entrada si es positiva, definida como \\( f(x) = \\max(0, x) \\)\" (Glorot, Xavier, Antoine Bordes, and Yoshua Bengio. 2011. \"Deep Sparse Rectifier Neural Networks\". Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics).\n",
    "\n",
    "## 3. Función Softmax\n",
    "\"La función softmax es una función de activación que convierte un vector de valores en una distribución de probabilidad, utilizada en la capa de salida de una red neuronal para clasificación multiclase\" (Bridle, John S. 1990. \"Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition\". In Neurocomputing).\n"
   ],
   "id": "2ad22103f2c04965"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8a0888d9e9987a7d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
